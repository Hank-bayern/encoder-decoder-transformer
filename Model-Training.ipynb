{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "725d8875",
   "metadata": {},
   "source": [
    "Encoder-Decoder Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d223afb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\__init__.py:2624\u001b[0m\n\u001b[0;32m   2620\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vmap \u001b[38;5;28;01mas\u001b[39;00m vmap\n\u001b[0;32m   2623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m-> 2624\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n\u001b[0;32m   2626\u001b[0m \u001b[38;5;66;03m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[0;32m   2627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTORCH_CUDA_SANITIZER\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\_meta_registrations.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SymBool, SymFloat, Tensor\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     13\u001b[0m     _add_op_to_registry,\n\u001b[0;32m     14\u001b[0m     _convert_out_params,\n\u001b[0;32m     15\u001b[0m     global_decomposition_table,\n\u001b[0;32m     16\u001b[0m     meta_table,\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpOverload\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _prim_elementwise_meta, ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\_decomp\\__init__.py:276\u001b[0m\n\u001b[0;32m    272\u001b[0m             decompositions\u001b[38;5;241m.\u001b[39mpop(op, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;66;03m# populate the table\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecompositions\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_refs\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcore_aten_decompositions\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomDecompTable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\_decomp\\decompositions.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_meta_registrations\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mprims\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\_prims\\__init__.py:2146\u001b[0m\n\u001b[0;32m   2138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2141\u001b[0m _copy_strided_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   2142\u001b[0m \u001b[38;5;124m  Copies the data in a to a new tensor, the new tensor has same shape with a size, but has different stride.\u001b[39m\n\u001b[0;32m   2143\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m-> 2146\u001b[0m copy_strided \u001b[38;5;241m=\u001b[39m _make_prim(\n\u001b[0;32m   2147\u001b[0m     schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy_strided(Tensor a, SymInt[] stride) -> Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2148\u001b[0m     meta\u001b[38;5;241m=\u001b[39m_copy_strided_meta,\n\u001b[0;32m   2149\u001b[0m     impl_aten\u001b[38;5;241m=\u001b[39m_copy_strided_aten,\n\u001b[0;32m   2150\u001b[0m     return_type\u001b[38;5;241m=\u001b[39mRETURN_TYPE\u001b[38;5;241m.\u001b[39mNEW,\n\u001b[0;32m   2151\u001b[0m     doc\u001b[38;5;241m=\u001b[39m_copy_strided_doc,\n\u001b[0;32m   2152\u001b[0m )\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_resize_meta\u001b[39m(a: TensorLikeType, shape: ShapeType):\n\u001b[0;32m   2156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\u001b[38;5;241m.\u001b[39mresize_(shape)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\_prims\\__init__.py:321\u001b[0m, in \u001b[0;36m_make_prim\u001b[1;34m(schema, return_type, meta, impl_aten, doc, tags, use_old_custom_ops_api, register_conj_neg_fallthrough)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     mutates_args \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    317\u001b[0m         arg\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m cpp_schema\u001b[38;5;241m.\u001b[39marguments\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m arg\u001b[38;5;241m.\u001b[39malias_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m arg\u001b[38;5;241m.\u001b[39malias_info\u001b[38;5;241m.\u001b[39mis_write\n\u001b[0;32m    320\u001b[0m     ]\n\u001b[1;32m--> 321\u001b[0m     prim_def \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlibrary\u001b[38;5;241m.\u001b[39mcustom_op(\n\u001b[0;32m    322\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprims::\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name,\n\u001b[0;32m    323\u001b[0m         _prim_impl,\n\u001b[0;32m    324\u001b[0m         mutates_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(mutates_args),\n\u001b[0;32m    325\u001b[0m         schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[0;32m    326\u001b[0m     )\n\u001b[0;32m    327\u001b[0m     prim_def\u001b[38;5;241m.\u001b[39mregister_fake(meta)\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;66;03m# all view ops get conj/neg fallthroughs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\_library\\custom_ops.py:174\u001b[0m, in \u001b[0;36mcustom_op\u001b[1;34m(name, fn, mutates_args, device_types, schema, tags)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner(fn)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\_library\\custom_ops.py:155\u001b[0m, in \u001b[0;36mcustom_op.<locals>.inner\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m    152\u001b[0m     schema_str \u001b[38;5;241m=\u001b[39m schema\n\u001b[0;32m    154\u001b[0m namespace, opname \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 155\u001b[0m result \u001b[38;5;241m=\u001b[39m CustomOpDef(namespace, opname, schema_str, fn, tags)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Check that schema's alias annotations match those of `mutates_args`.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     expected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\_library\\custom_ops.py:213\u001b[0m, in \u001b[0;36mCustomOpDef.__init__\u001b[1;34m(self, namespace, name, schema, fn, tags)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autocast_cpu_dtype: Optional[_dtype] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib \u001b[38;5;241m=\u001b[39m get_library_allowing_overwrite(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_namespace, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name)\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_to_dispatcher(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tags)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disabled_kernel: \u001b[38;5;28mset\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    215\u001b[0m OPDEFS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qualname] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\_library\\custom_ops.py:636\u001b[0m, in \u001b[0;36mCustomOpDef._register_to_dispatcher\u001b[1;34m(self, tags)\u001b[0m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_abstract_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    634\u001b[0m lib\u001b[38;5;241m.\u001b[39m_register_fake(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, fake_impl, _stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m--> 636\u001b[0m autograd_impl \u001b[38;5;241m=\u001b[39m autograd\u001b[38;5;241m.\u001b[39mmake_autograd_impl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opoverload, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    637\u001b[0m lib\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, autograd_impl, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutograd\u001b[39m\u001b[38;5;124m\"\u001b[39m, with_keyset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    639\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opoverload\u001b[38;5;241m.\u001b[39m_schema\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\_library\\autograd.py:28\u001b[0m, in \u001b[0;36mmake_autograd_impl\u001b[1;34m(op, info)\u001b[0m\n\u001b[0;32m     24\u001b[0m name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeneratedBackwardFor_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop\u001b[38;5;241m.\u001b[39m_namespace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop\u001b[38;5;241m.\u001b[39m_opname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop\u001b[38;5;241m.\u001b[39m_overloadname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m has_kwarg_only_args \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mhas_kwarg_only_args(op\u001b[38;5;241m.\u001b[39m_schema)\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMetadata\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m     keyset: _C\u001b[38;5;241m.\u001b[39mDispatchKeySet\n\u001b[0;32m     31\u001b[0m     keyword_only_args: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\dataclasses.py:1275\u001b[0m, in \u001b[0;36mdataclass\u001b[1;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[0;32m   1272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrap\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;66;03m# We're called as @dataclass without parens.\u001b[39;00m\n\u001b[1;32m-> 1275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\dataclasses.py:1265\u001b[0m, in \u001b[0;36mdataclass.<locals>.wrap\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[1;32m-> 1265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _process_class(\u001b[38;5;28mcls\u001b[39m, init, \u001b[38;5;28mrepr\u001b[39m, eq, order, unsafe_hash,\n\u001b[0;32m   1266\u001b[0m                           frozen, match_args, kw_only, slots,\n\u001b[0;32m   1267\u001b[0m                           weakref_slot)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\dataclasses.py:1056\u001b[0m, in \u001b[0;36m_process_class\u001b[1;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;66;03m# Include InitVars and regular fields (so, not ClassVars).  This is\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;66;03m# initialized here, outside of the \"if init:\" test, because std_init_fields\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# is used with match_args, below.\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m all_init_fields \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m   1054\u001b[0m                    \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_field_type \u001b[38;5;129;01min\u001b[39;00m (_FIELD, _FIELD_INITVAR)]\n\u001b[0;32m   1055\u001b[0m (std_init_fields,\n\u001b[1;32m-> 1056\u001b[0m  kw_only_init_fields) \u001b[38;5;241m=\u001b[39m _fields_in_init_order(all_init_fields)\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m init:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;66;03m# Does this class have a post-init function?\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m     has_post_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, _POST_INIT_NAME)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\dataclasses.py:433\u001b[0m, in \u001b[0;36m_fields_in_init_order\u001b[1;34m(fields)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fields_in_init_order\u001b[39m(fields):\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;66;03m# Returns the fields as __init__ will output them.  It returns 2 tuples:\u001b[39;00m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;66;03m# the first for normal args, and the second for keyword args.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mtuple\u001b[39m(f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39minit \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mkw_only),\n\u001b[1;32m--> 433\u001b[0m             \u001b[38;5;28mtuple\u001b[39m(f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39minit \u001b[38;5;129;01mand\u001b[39;00m f\u001b[38;5;241m.\u001b[39mkw_only)\n\u001b[0;32m    434\u001b[0m             )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\dataclasses.py:433\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fields_in_init_order\u001b[39m(fields):\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;66;03m# Returns the fields as __init__ will output them.  It returns 2 tuples:\u001b[39;00m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;66;03m# the first for normal args, and the second for keyword args.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mtuple\u001b[39m(f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39minit \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mkw_only),\n\u001b[1;32m--> 433\u001b[0m             \u001b[38;5;28mtuple\u001b[39m(f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39minit \u001b[38;5;129;01mand\u001b[39;00m f\u001b[38;5;241m.\u001b[39mkw_only)\n\u001b[0;32m    434\u001b[0m             )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import math\n",
    "import csv\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --------------------- Dataset ---------------------\n",
    "class SequenceClassificationDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "\n",
    "        with open(csv_file, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                input_str, output_str = row[0].split('][')\n",
    "                input_list = ast.literal_eval(input_str + ']')\n",
    "                output_list = ast.literal_eval('[' + output_str)\n",
    "                self.inputs.append(input_list)\n",
    "                self.outputs.append(output_list)\n",
    "\n",
    "        self.inputs = torch.tensor(self.inputs, dtype=torch.long)\n",
    "        self.outputs = torch.tensor(self.outputs, dtype=torch.long)\n",
    "        self.vocab_size = max(torch.max(self.inputs), torch.max(self.outputs)) + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.outputs[idx]\n",
    "\n",
    "# ------------------ Token Weights ------------------\n",
    "def compute_token_weights(dataset, vocab_size):\n",
    "    token_counts = torch.bincount(dataset.outputs.view(-1), minlength=vocab_size)\n",
    "    freq = token_counts.float() + 1e-6\n",
    "    weights = 1.0 / torch.sqrt(freq)\n",
    "    weights = weights / weights.sum() * vocab_size\n",
    "    return weights.to(device)\n",
    "\n",
    "# --------------------- Positional Encoding ---------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# --------------------- Seq2Seq Transformer ---------------------\n",
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=4, num_encoder_layers=4, num_decoder_layers=4, dim_feedforward=1024, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.pos_decoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=False)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=False)\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src).transpose(0, 1)\n",
    "        tgt = self.embedding(tgt).transpose(0, 1)\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.pos_decoder(tgt)\n",
    "\n",
    "        memory = self.encoder(src)\n",
    "        output = self.decoder(tgt, memory)\n",
    "        return self.output_layer(output).transpose(0, 1)\n",
    "\n",
    "# --------------------- Training ---------------------\n",
    "def train_model(model, train_loader, val_loader, weights, epochs=50, patience=5):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights, ignore_index=2)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, total_token, correct_token, exact_match = 0, 0, 0, 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            tgt_input = targets[:, :-1]\n",
    "            tgt_output = targets[:, 1:]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                logits = model(inputs, tgt_input)\n",
    "                loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_output.reshape(-1))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            mask = tgt_output != 2  # 2 is the index of <pad>\n",
    "            correct_token += ((preds == tgt_output) & mask).sum().item()\n",
    "            total_token += mask.sum().item()\n",
    "            match = (preds == tgt_output) | ~mask  # The position of <pad> does not need to be predicted correctly\n",
    "            exact_match += match.all(dim=1).sum().item()\n",
    "\n",
    "        token_acc = correct_token / total_token\n",
    "        seq_acc = exact_match / len(train_loader.dataset)\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_total, val_correct, val_exact = 0, 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                tgt_input = targets[:, :-1]\n",
    "                tgt_output = targets[:, 1:]\n",
    "\n",
    "                with autocast():\n",
    "                    logits = model(inputs, tgt_input)\n",
    "                    loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_output.reshape(-1))\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                mask = tgt_output != 2\n",
    "                val_correct += ((preds == tgt_output) & mask).sum().item()\n",
    "                val_total += mask.sum().item()\n",
    "                match = (preds == tgt_output) | ~mask  #<pad> position is not counted towards matching requirement\n",
    "                val_exact += match.all(dim=1).sum().item()\n",
    "\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_token_acc = val_correct / val_total\n",
    "        val_seq_acc = val_exact / len(val_loader.dataset)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'E:/Language Model Project/final-model/best_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "            \n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | LR: {lr:.5f}\")\n",
    "        print(f\"    TokenAcc: train={token_acc:.4f}, val={val_token_acc:.4f} | SeqAcc: train={seq_acc:.4f}, val={val_seq_acc:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('loss_curve.png')\n",
    "    plt.show()\n",
    "\n",
    "# --------------------- Main ---------------------\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = SequenceClassificationDataset('E:/Language Model Project/final-model/Data/train_15-20_10_shuffle.csv')#_20_shuffle\n",
    "    weights = compute_token_weights(dataset, dataset.vocab_size)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    print(\"dataset had been loaded\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=128, num_workers=0)\n",
    "\n",
    "    model = TransformerSeq2Seq(\n",
    "        vocab_size=dataset.vocab_size,\n",
    "        d_model=256,\n",
    "        nhead=4,\n",
    "        num_encoder_layers=4,\n",
    "        num_decoder_layers=4,\n",
    "        dim_feedforward=1024,\n",
    "        dropout=0.3\n",
    "    )\n",
    "\n",
    "    train_model(model, train_loader, val_loader, weights, epochs=50, patience=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
