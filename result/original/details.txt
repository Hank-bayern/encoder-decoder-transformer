model = TransformerSeq2Seq(
        vocab_size=dataset.vocab_size,
        d_model=128,
        nhead=4,
        num_encoder_layers=1,
        num_decoder_layers=1,
        dim_feedforward=512,
        dropout=0.1
    )
    criterion = nn.CrossEntropyLoss(weight=weights, ignore_index=2)
    optimizer = optim.AdamW(model.parameters(), lr=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)